from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from datetime import datetime
import httpx
import urllib.parse
import re
import difflib

app = FastAPI(title="File Analysis Service", version="1.0.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


class AnalysisRequest(BaseModel):
    work_id: int
    student_name: str
    assignment_id: str
    file_name: str
    file_hash: str
    file_content: str = ""


# –•—Ä–∞–Ω–∏–ª–∏—â–µ –¥–∞–Ω–Ω—ã—Ö
all_works_cache = []  # –î–ª—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
reports_db = []
next_report_id = 1
FILE_SERVICE_URL = "http://file-storing:8001"
PLAGIARISM_THRESHOLD = 0.7  # –ü–æ—Ä–æ–≥ 70% –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–ª–∞–≥–∏–∞—Ç–∞


def calculate_text_similarity_advanced(text1: str, text2: str) -> float:
    """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ö–æ–∂–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–æ–≤"""
    if not text1 or not text2:
        return 0.0

    # 1. –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
    text1 = text1.lower().strip()
    text2 = text2.lower().strip()

    # 2. –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    text1 = ' '.join(text1.split())
    text2 = ' '.join(text2.split())

    # 3. –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ä–∞–∑–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏
    similarities = []

    # –ú–µ—Ç–æ–¥ 1: SequenceMatcher
    seq_similarity = difflib.SequenceMatcher(None, text1, text2).ratio()
    similarities.append(seq_similarity)

    # –ú–µ—Ç–æ–¥ 2: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ —Ç–æ–∫–µ–Ω–∞–º
    words1 = set(re.findall(r'\b\w+\b', text1))
    words2 = set(re.findall(r'\b\w+\b', text2))

    if words1 and words2:
        common_words = words1.intersection(words2)
        all_words = words1.union(words2)
        token_similarity = len(common_words) / len(all_words) if all_words else 0
        similarities.append(token_similarity)

    # –ú–µ—Ç–æ–¥ 3: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ —à–∏–Ω–≥–ª–∞–º
    def get_shingles(text, n=3):
        words = text.split()
        shingles = set()
        for i in range(len(words) - n + 1):
            shingle = ' '.join(words[i:i + n])
            shingles.add(shingle)
        return shingles

    shingles1 = get_shingles(text1, 3)
    shingles2 = get_shingles(text2, 3)

    if shingles1 and shingles2:
        common_shingles = shingles1.intersection(shingles2)
        all_shingles = shingles1.union(shingles2)
        shingle_similarity = len(common_shingles) / len(all_shingles) if all_shingles else 0
        similarities.append(shingle_similarity)

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
    return sum(similarities) / len(similarities) if similarities else 0.0


def calculate_text_similarity(text1: str, text2: str) -> float:
    """–í—ã—á–∏—Å–ª—è–µ—Ç –ø—Ä–æ—Ü–µ–Ω—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–≤—É—Ö —Ç–µ–∫—Å—Ç–æ–≤"""
    if not text1 or not text2:
        return 0.0

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º SequenceMatcher –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤
    similarity = difflib.SequenceMatcher(None, text1, text2).ratio()
    return similarity


@app.post("/analyze")
async def analyze_file(request: AnalysisRequest):
    global next_report_id, all_works_cache

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª–∞—Å—å –ª–∏ —É–∂–µ —Ä–∞–±–æ—Ç–∞
    for report in reports_db:
        if report["work_id"] == request.work_id:
            return {"message": "–ê–Ω–∞–ª–∏–∑ —É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω", "report_id": report["id"]}

    try:
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Ä–∞–±–æ—Ç—ã –∏–∑ File Storing Service
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(f"{FILE_SERVICE_URL}/works")

            max_similarity = 0.0
            original_author = None
            matched_work_id = None
            matched_file_name = None

            if response.status_code == 200:
                data = response.json()
                all_works = data.get("works", [])

                print(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–ª–∞–≥–∏–∞—Ç–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã {request.work_id}. –í—Å–µ–≥–æ —Ä–∞–±–æ—Ç: {len(all_works)}")
                print(f"üìù –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {len(request.file_content)} —Å–∏–º–≤–æ–ª–æ–≤")

                # –ò—â–µ–º —Å–∞–º—É—é –ø–æ—Ö–æ–∂—É—é —Ä–∞–±–æ—Ç—É
                for work in all_works:
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ–∫—É—â—É—é —Ä–∞–±–æ—Ç—É –∏ —Ä–∞–±–æ—Ç—ã —Ç–æ–≥–æ –∂–µ —Å—Ç—É–¥–µ–Ω—Ç–∞
                    if work["id"] == request.work_id or work["student_name"] == request.student_name:
                        continue

                    # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –∫—ç—à–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
                    work_text = ""
                    for cached_work in all_works_cache:
                        if cached_work["id"] == work["id"]:
                            work_text = cached_work.get("text", "")
                            break

                    # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–µ—Ç –≤ –∫—ç—à–µ, –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏–∑ file-storing
                    if not work_text and request.file_content:
                        try:
                            file_response = await client.get(f"{FILE_SERVICE_URL}/download/{work['id']}")
                            if file_response.status_code == 200:
                                # –ó–¥–µ—Å—å –Ω—É–∂–Ω–∞ –ª–æ–≥–∏–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞
                                # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —á—Ç–æ —ç—Ç–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª
                                try:
                                    work_text = file_response.content.decode('utf-8', errors='ignore')
                                except:
                                    work_text = ""
                        except:
                            work_text = ""

                    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç—ã
                    if request.file_content and work_text:
                        similarity = calculate_text_similarity(request.file_content, work_text)
                        print(f"  ‚Ä¢ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —Ä–∞–±–æ—Ç–æ–π {work['id']} ({work['student_name']}): {similarity:.2%}")

                        if similarity > max_similarity:
                            max_similarity = similarity
                            original_author = work["student_name"]
                            matched_work_id = work["id"]
                            matched_file_name = work["file_name"]

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—Å—Ç —Ç–µ–∫—É—â–µ–π —Ä–∞–±–æ—Ç—ã –≤ –∫—ç—à
            if request.file_content:
                current_work_data = {
                    "id": request.work_id,
                    "student_name": request.student_name,
                    "text": request.file_content,
                    "created_at": datetime.now().isoformat()
                }
                all_works_cache.append(current_work_data)

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            plagiarism_score = max_similarity
            is_plagiarism = plagiarism_score > PLAGIARISM_THRESHOLD

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±–ª–∞–∫–∞ —Å–ª–æ–≤
        word_cloud_url = None
        if request.file_content:
            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 1000 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –æ–±–ª–∞–∫–∞ —Å–ª–æ–≤
            text_for_cloud = request.file_content[:1000] if len(request.file_content) > 1000 else request.file_content

            words = re.findall(r'\b\w+\b', text_for_cloud.lower())

            if words:
                # –ë–µ—Ä–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –∏ —Å–æ–µ–¥–∏–Ω—è–µ–º —á–µ—Ä–µ–∑ –ø—Ä–æ–±–µ–ª
                unique_words = list(set(words))
                cloud_text = ' '.join(unique_words[:50])  # –ú–∞–∫—Å–∏–º—É–º 50 —Å–ª–æ–≤

                # –ö–æ–¥–∏—Ä—É–µ–º –¥–ª—è URL
                encoded_text = urllib.parse.quote(cloud_text)
                word_cloud_url = f"https://quickchart.io/wordcloud?text={encoded_text}&width=1000&height=800&format=png"

        # –°–æ–∑–¥–∞–µ–º –æ—Ç—á–µ—Ç
        report = {
            "id": next_report_id,
            "work_id": request.work_id,
            "student_name": request.student_name,
            "assignment_id": request.assignment_id,
            "file_name": request.file_name,
            "is_plagiarism": is_plagiarism,
            "plagiarism_score": plagiarism_score,  # –ü—Ä–æ—Ü–µ–Ω—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
            "original_author": original_author,
            "matched_work_id": matched_work_id,
            "matched_file_name": matched_file_name,
            "similarity_percentage": round(plagiarism_score * 100, 2),
            "word_cloud_url": word_cloud_url,
            "file_hash": request.file_hash,
            "created_at": datetime.now().isoformat()
        }

        reports_db.append(report)
        next_report_id += 1

        if is_plagiarism:
            print(f"‚ö†Ô∏è  –û–ë–ù–ê–†–£–ñ–ï–ù –ü–õ–ê–ì–ò–ê–¢! –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ: {plagiarism_score:.2%} —Å —Ä–∞–±–æ—Ç–æ–π {matched_work_id}")
        else:
            print(f"‚úÖ –ê–Ω–∞–ª–∏–∑ –¥–ª—è —Ä–∞–±–æ—Ç—ã {request.work_id}: —Å—Ö–æ–∂–µ—Å—Ç—å {plagiarism_score:.2%}")

        return {
            "message": "–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω",
            "work_id": request.work_id,
            "report_id": report["id"],
            "is_plagiarism": is_plagiarism,
            "similarity_percentage": round(plagiarism_score * 100, 2)
        }

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}")


@app.get("/works/{work_id}/report")
async def get_report(work_id: int):
    for report in reports_db:
        if report["work_id"] == work_id:
            return report
    raise HTTPException(status_code=404, detail="–û—Ç—á–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")


@app.get("/assignment/{assignment_id}/reports")
async def get_assignment_reports(assignment_id: str):
    result = [r for r in reports_db if r["assignment_id"] == assignment_id]
    return {"reports": result, "total": len(result)}


@app.get("/health")
async def health():
    return {"status": "healthy", "service": "file-analysis"}


@app.get("/debug/works")
async def debug_works():
    return {"works": all_works_cache, "total": len(all_works_cache)}